{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Models and Vectorization Strategies for Text Classification\n",
    "\n",
    "This try-it focuses on weighing the positives and negatives of different estimators and vectorization strategies for a text classification problem.  In order to consider each of these components, you should make use of the `Pipeline` and `GridSearchCV` objects in scikitlearn to try different combinations of vectorizers with different estimators.  For each of these, you also want to use the `.cv_results_` to examine the time for the estimator to fit the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Data\n",
    "\n",
    "The dataset below is from [kaggle]() and contains a dataset named the \"ColBert Dataset\" created for this [paper](https://arxiv.org/pdf/2004.12765.pdf).  You are to use the text column to classify whether or not the text was humorous.  It is loaded and displayed below.\n",
    "\n",
    "**Note:** The original dataset contains 200K rows of data. It is best to try to use the full dtaset. If the original dataset is too large for your computer, please use the 'dataset-minimal.csv', which has been reduced to 100K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/mma0812/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/mma0812/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/mma0812/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "#NLP libraies\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer,TfidfTransformer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime as dt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('text_data/dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>humor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Joe biden rules out 2020 bid: 'guys, i'm not r...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Watch: darvish gave hitter whiplash with slow ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What do you call a turtle without its shell? d...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5 reasons the 2016 election feels so personal</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pasco police shot mexican migrant from behind,...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  humor\n",
       "0  Joe biden rules out 2020 bid: 'guys, i'm not r...  False\n",
       "1  Watch: darvish gave hitter whiplash with slow ...  False\n",
       "2  What do you call a turtle without its shell? d...   True\n",
       "3      5 reasons the 2016 election feels so personal  False\n",
       "4  Pasco police shot mexican migrant from behind,...  False"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task\n",
    "\n",
    "\n",
    "**Text preprocessing:** As a pre-processing step, perform both `stemming` and `lemmatizing` to normalize your text before classifying. For each technique use both the `CountVectorize`r and `TfidifVectorizer` and use options for stop words and max features to prepare the text data for your estimator.\n",
    "\n",
    "**Classification:** Once you have prepared the text data with stemming lemmatizing techniques, consider `LogisticRegression`, `DecisionTreeClassifier`, and `MultinomialNB` as classification algorithms for the data. Compare their performance in terms of accuracy and speed.\n",
    "\n",
    "Share the results of your best classifier in the form of a table with the best version of each estimator, a dictionary of the best parameters and the best score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 200000 entries, 0 to 199999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    200000 non-null  object\n",
      " 1   humor   200000 non-null  bool  \n",
      "dtypes: bool(1), object(1)\n",
      "memory usage: 1.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemmer(text):\n",
    "    '''\n",
    "    This function takes in a string of text and returns\n",
    "    a list of stemmered text.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    text: str\n",
    "        string of text to be stemmered\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "       string of stemmered words from text input\n",
    "    '''\n",
    "    stem = PorterStemmer()\n",
    "    return ' '.join([stem.stem(w) for w in word_tokenize(text)])\n",
    "\n",
    "def lemmatiz(text):\n",
    "    '''\n",
    "    This function takes in a string of text and returns\n",
    "    a list of lemmatized text.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    text: str\n",
    "        string of text to be lemmatized\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "       string of lemmatized words from text input\n",
    "    '''\n",
    "    lemma = WordNetLemmatizer()\n",
    "    return ' '.join([lemma.lemmatize(w) for w in word_tokenize(text)]) # Tokenize and then lemmatize. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_df = df.copy()\n",
    "stemmed_df['text'] = df['text'].apply(stemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>humor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>joe biden rule out 2020 bid : 'guy , i 'm not ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>watch : darvish gave hitter whiplash with slow...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what do you call a turtl without it shell ? de...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5 reason the 2016 elect feel so person</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pasco polic shot mexican migrant from behind ,...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  humor\n",
       "0  joe biden rule out 2020 bid : 'guy , i 'm not ...  False\n",
       "1  watch : darvish gave hitter whiplash with slow...  False\n",
       "2  what do you call a turtl without it shell ? de...   True\n",
       "3             5 reason the 2016 elect feel so person  False\n",
       "4  pasco polic shot mexican migrant from behind ,...  False"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem_df = pd.DataFrame(stemmed_df)\n",
    "stem_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>humor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Joe biden rule out 2020 bid : 'guys , i 'm not...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Watch : darvish gave hitter whiplash with slow...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What do you call a turtle without it shell ? d...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5 reason the 2016 election feel so personal</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pasco police shot mexican migrant from behind ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  humor\n",
       "0  Joe biden rule out 2020 bid : 'guys , i 'm not...  False\n",
       "1  Watch : darvish gave hitter whiplash with slow...  False\n",
       "2  What do you call a turtle without it shell ? d...   True\n",
       "3        5 reason the 2016 election feel so personal  False\n",
       "4  Pasco police shot mexican migrant from behind ...  False"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma_df = df.copy()\n",
    "lemma_df['text'] = df['text'].apply(lemmatiz)\n",
    "lemma_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Joe biden rules out 2020 bid: 'guys, i'm not r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Watch: darvish gave hitter whiplash with slow ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What do you call a turtle without its shell? d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5 reasons the 2016 election feels so personal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pasco police shot mexican migrant from behind,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199995</th>\n",
       "      <td>Conor maynard seamlessly fits old-school r&amp;b h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199996</th>\n",
       "      <td>How to you make holy water? you boil the hell ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199997</th>\n",
       "      <td>How many optometrists does it take to screw in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199998</th>\n",
       "      <td>Mcdonald's will officially kick off all-day br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199999</th>\n",
       "      <td>An irish man walks on the street and ignores a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text\n",
       "0       Joe biden rules out 2020 bid: 'guys, i'm not r...\n",
       "1       Watch: darvish gave hitter whiplash with slow ...\n",
       "2       What do you call a turtle without its shell? d...\n",
       "3           5 reasons the 2016 election feels so personal\n",
       "4       Pasco police shot mexican migrant from behind,...\n",
       "...                                                   ...\n",
       "199995  Conor maynard seamlessly fits old-school r&b h...\n",
       "199996  How to you make holy water? you boil the hell ...\n",
       "199997  How many optometrists does it take to screw in...\n",
       "199998  Mcdonald's will officially kick off all-day br...\n",
       "199999  An irish man walks on the street and ignores a...\n",
       "\n",
       "[200000 rows x 1 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.drop('humor', axis = 1)\n",
    "y = df['humor']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X['text'], y, test_size=0.25, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### W/o Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>0000251</th>\n",
       "      <th>0000ff</th>\n",
       "      <th>0001</th>\n",
       "      <th>000th</th>\n",
       "      <th>001</th>\n",
       "      <th>00100</th>\n",
       "      <th>002</th>\n",
       "      <th>00463</th>\n",
       "      <th>...</th>\n",
       "      <th>αστυνομίας</th>\n",
       "      <th>διαδηλωτών</th>\n",
       "      <th>κάιρο</th>\n",
       "      <th>και</th>\n",
       "      <th>μεταξύ</th>\n",
       "      <th>νεκρός</th>\n",
       "      <th>σε</th>\n",
       "      <th>στο</th>\n",
       "      <th>συγκρούσεις</th>\n",
       "      <th>ᵒᴥᵒᶅ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>149995</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149996</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149997</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149998</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149999</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 53889 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        00  000  0000251  0000ff  0001  000th  001  00100  002  00463  ...  \\\n",
       "149995   0    0        0       0     0      0    0      0    0      0  ...   \n",
       "149996   0    0        0       0     0      0    0      0    0      0  ...   \n",
       "149997   0    0        0       0     0      0    0      0    0      0  ...   \n",
       "149998   0    0        0       0     0      0    0      0    0      0  ...   \n",
       "149999   0    0        0       0     0      0    0      0    0      0  ...   \n",
       "\n",
       "        αστυνομίας  διαδηλωτών  κάιρο  και  μεταξύ  νεκρός  σε  στο  \\\n",
       "149995           0           0      0    0       0       0   0    0   \n",
       "149996           0           0      0    0       0       0   0    0   \n",
       "149997           0           0      0    0       0       0   0    0   \n",
       "149998           0           0      0    0       0       0   0    0   \n",
       "149999           0           0      0    0       0       0   0    0   \n",
       "\n",
       "        συγκρούσεις  ᵒᴥᵒᶅ  \n",
       "149995            0     0  \n",
       "149996            0     0  \n",
       "149997            0     0  \n",
       "149998            0     0  \n",
       "149999            0     0  \n",
       "\n",
       "[5 rows x 53889 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvect = CountVectorizer()\n",
    "cvect.fit_transform(X_train)\n",
    "pd.DataFrame(dtm.toarray(), columns = cvect.get_feature_names()).tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('cvect', CountVectorizer()),\n",
    "                ('lgr', MultinomialNB())])\n",
    "pipe.fit(X_train, y_train)\n",
    "test_acc = pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'cvect__max_features': [100, 300, 1000, 2000, 4000, 7000],\n",
    "         'cvect__stop_words': ['english', None]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(pipe, param_grid=params)\n",
    "grid.fit(X_train, y_train)\n",
    "acc = grid.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'cvect__max_features': 7000, 'cvect__stop_words': None}, 0.9084533333333333)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_, grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_s = stem_df.drop('humor', axis = 1)\n",
    "y_s = stem_df['humor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_s, X_test_s, y_train_s, y_test_s = train_test_split(X_s['text'], y_s, test_size=0.25, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_stem_countvec_mnb = Pipeline([('cvect', CountVectorizer()),\n",
    "                ('lgr', MultinomialNB())])\n",
    "pipe_stem_countvec_mnb.fit(X_train_s, y_train_s)\n",
    "test_acc_s = pipe_stem_countvec_mnb.score(X_test_s, y_test_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = dt.now()\n",
    "\n",
    "grid_s = GridSearchCV(pipe_stem_countvec_mnb, param_grid=params)\n",
    "grid_s.fit(X_train_s, y_train_s)\n",
    "\n",
    "running_secs_nb = (dt.now() - start).seconds\n",
    "\n",
    "acc_s = grid_s.score(X_test_s, y_test_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'cvect__max_features': 7000, 'cvect__stop_words': None},\n",
       " 0.9063333333333332,\n",
       " GridSearchCV(estimator=Pipeline(steps=[('cvect', CountVectorizer()),\n",
       "                                        ('lgr', MultinomialNB())]),\n",
       "              param_grid={'cvect__max_features': [100, 300, 1000, 2000, 4000,\n",
       "                                                  7000],\n",
       "                          'cvect__stop_words': ['english', None]}),\n",
       " 88)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_s.best_params_, grid_s.best_score_,grid_s, running_secs_nb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lr = lemma_df.drop('humor', axis = 1)\n",
    "y_lr = lemma_df['humor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lr, X_test_lr, y_train_lr, y_test_lr = train_test_split(X_lr['text'], y_lr, test_size=0.25, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_stem_tfidf_mnb = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                ('mnb', MultinomialNB())])\n",
    "pipe_stem_tfidf_mnb.fit(X_train_lr, y_train_lr)\n",
    "test_acc_lr_mnb = pipe_stem_tfidf_mnb.score(X_test_lr, y_test_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_s_mnb = {'tfidf__max_features': [1000, 2000, 4000, 7000],\n",
    "         'tfidf__stop_words': ['english', None]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'tfidf__max_features': 7000, 'tfidf__stop_words': None},\n",
       " 0.9040333333333332,\n",
       " 0.90494,\n",
       " 0.912,\n",
       " 62)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = dt.now()\n",
    "# process stuff\n",
    "grid_tf_mnb = GridSearchCV(pipe_stem_tfidf_mnb, param_grid=params_s_mnb)\n",
    "grid_tf_mnb.fit(X_train_lr, y_train_lr)\n",
    "acc_tf_mnb = grid_tf_mnb.score(X_test_lr, y_test_lr)\n",
    "\n",
    "running_secs_nb = (dt.now() - start).seconds\n",
    "\n",
    "grid_tf_mnb.best_params_, grid_tf_mnb.best_score_, test_acc_lr_mnb, running_secs_nb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With MultiNominal Hyper Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_s_mnb1 = {'tfidf__max_features': [1000, 2000, 4000, 7000],\n",
    "         'tfidf__stop_words': ['english', None], \n",
    "                'mnb__alpha': np.linspace(0.5, 1.5, 6),\n",
    "                'mnb__fit_prior': [True, False],}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'mnb__alpha': 1.5,\n",
       "  'mnb__fit_prior': False,\n",
       "  'tfidf__max_features': 7000,\n",
       "  'tfidf__stop_words': None},\n",
       " 0.9041266666666667,\n",
       " 0.90472,\n",
       " 0.912,\n",
       " 733)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = dt.now()\n",
    "# process stuff\n",
    "grid_tf_mnb = GridSearchCV(pipe_stem_tfidf_mnb, param_grid=params_s_mnb1)\n",
    "grid_tf_mnb.fit(X_train_lr, y_train_lr)\n",
    "acc_tf_mnb = grid_tf_mnb.score(X_test_lr, y_test_lr)\n",
    "\n",
    "running_secs_nb = (dt.now() - start).seconds\n",
    "\n",
    "grid_tf_mnb.best_params_, grid_tf_mnb.best_score_, acc_tf_mnb, test_acc_lr_mnb, running_secs_nb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_stem_countvec_lr = Pipeline([('cvect', CountVectorizer()),\n",
    "                ('lgr', LogisticRegression(max_iter=10000))])\n",
    "pipe_stem_countvec_lr.fit(X_train_s, y_train_s)\n",
    "test_acc_s_lr = pipe_stem_countvec_lr.score(X_test_s, y_test_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_s_lr = {'cvect__max_features': [100, 300, 1000, 2000, 4000, 7000],\n",
    "         'cvect__stop_words': ['english', None]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = dt.now()\n",
    "\n",
    "grid_s_lr = GridSearchCV(pipe_stem_countvec_lr, param_grid=params_s_lr)\n",
    "grid_s_lr.fit(X_train_s, y_train_s)\n",
    "\n",
    "running_secs = (dt.now() - start).seconds\n",
    "\n",
    "acc_s_lr = grid_s.score(X_test_s, y_test_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'cvect__max_features': 7000, 'cvect__stop_words': None},\n",
       " 0.9230599999999999,\n",
       " 153)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_s_lr.best_params_, grid_s_lr.best_score_, running_secs\n",
    "#,grid_s.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With lemmitazation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lr = lemma_df.drop('humor', axis = 1)\n",
    "y_lr = lemma_df['humor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lr, X_test_lr, y_train_lr, y_test_lr = train_test_split(X_lr['text'], y_lr, test_size=0.25, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_stem_tfidf_lr = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                ('lgr', LogisticRegression(max_iter=10000))])\n",
    "pipe_stem_tfidf_lr.fit(X_train_lr, y_train_lr)\n",
    "test_acc_s_lr = pipe_stem_tfidf_lr.score(X_test_lr, y_test_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_s_lr = {'tfidf__max_features': [100, 300, 1000, 2000, 4000, 7000],\n",
    "         'tfidf__stop_words': ['english', None]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'tfidf__max_features': 7000, 'tfidf__stop_words': None},\n",
       " 0.9182733333333333,\n",
       " 0.9214,\n",
       " 132)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = dt.now()\n",
    "# process stuff\n",
    "grid_tf_lr = GridSearchCV(pipe_stem_tfidf_lr, param_grid=params_s_lr)\n",
    "grid_tf_lr.fit(X_train_lr, y_train_lr)\n",
    "acc_tf_lr = grid_tf_lr.score(X_test_lr, y_test_lr)\n",
    "\n",
    "running_secs = (dt.now() - start).seconds\n",
    "\n",
    "grid_tf_lr.best_params_, grid_tf_lr.best_score_, acc_tf_lr, running_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Stemmer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_stem_countvec_dt = Pipeline([('cvect', CountVectorizer()),\n",
    "                ('dt', DecisionTreeClassifier())])\n",
    "pipe_stem_countvec_dt.fit(X_train_s, y_train_s)\n",
    "test_acc_s = pipe_stem_countvec_dt.score(X_test_s, y_test_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_s_dt = {'cvect__max_features': [100, 500, 1000, 2000],\n",
    "         'cvect__stop_words': ['english', None],\n",
    "              'dt__criterion': ['gini', 'entropy']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'cvect__max_features': 2000,\n",
       "  'cvect__stop_words': None,\n",
       "  'dt__criterion': 'entropy'},\n",
       " 0.8654333333333334,\n",
       " {'mean_fit_time': array([ 3.61378303,  3.40246325,  8.97969937,  8.687149  , 13.92021894,\n",
       "         13.91123772, 18.56151605, 17.50884995, 26.96916919, 24.99748368,\n",
       "         21.57136016, 19.85271945, 21.35472646, 21.94333692, 25.96873889,\n",
       "         20.94574633]),\n",
       "  'std_fit_time': array([0.37074641, 0.11862086, 0.06792432, 0.1619717 , 0.16884839,\n",
       "         0.25712764, 0.37489712, 1.23526311, 6.4511432 , 5.85126921,\n",
       "         0.82489619, 0.61415141, 0.69251915, 1.1851206 , 0.64540662,\n",
       "         0.66847812]),\n",
       "  'mean_score_time': array([0.27465858, 0.26676159, 0.24453754, 0.24960918, 0.27030754,\n",
       "         0.27179928, 0.2670742 , 0.26704717, 0.57527714, 0.37485366,\n",
       "         0.28543701, 0.28565273, 0.27334518, 0.29322724, 0.2906312 ,\n",
       "         0.28169866]),\n",
       "  'std_score_time': array([0.06822856, 0.04402015, 0.00261507, 0.00969568, 0.00961976,\n",
       "         0.01002208, 0.00352887, 0.00454542, 0.2299458 , 0.17416466,\n",
       "         0.01230474, 0.01221442, 0.0009727 , 0.01180153, 0.0178415 ,\n",
       "         0.01572985]),\n",
       "  'param_cvect__max_features': masked_array(data=[100, 100, 100, 100, 500, 500, 500, 500, 1000, 1000,\n",
       "                     1000, 1000, 2000, 2000, 2000, 2000],\n",
       "               mask=[False, False, False, False, False, False, False, False,\n",
       "                     False, False, False, False, False, False, False, False],\n",
       "         fill_value='?',\n",
       "              dtype=object),\n",
       "  'param_cvect__stop_words': masked_array(data=['english', 'english', None, None, 'english', 'english',\n",
       "                     None, None, 'english', 'english', None, None,\n",
       "                     'english', 'english', None, None],\n",
       "               mask=[False, False, False, False, False, False, False, False,\n",
       "                     False, False, False, False, False, False, False, False],\n",
       "         fill_value='?',\n",
       "              dtype=object),\n",
       "  'param_dt__criterion': masked_array(data=['gini', 'entropy', 'gini', 'entropy', 'gini',\n",
       "                     'entropy', 'gini', 'entropy', 'gini', 'entropy',\n",
       "                     'gini', 'entropy', 'gini', 'entropy', 'gini',\n",
       "                     'entropy'],\n",
       "               mask=[False, False, False, False, False, False, False, False,\n",
       "                     False, False, False, False, False, False, False, False],\n",
       "         fill_value='?',\n",
       "              dtype=object),\n",
       "  'params': [{'cvect__max_features': 100,\n",
       "    'cvect__stop_words': 'english',\n",
       "    'dt__criterion': 'gini'},\n",
       "   {'cvect__max_features': 100,\n",
       "    'cvect__stop_words': 'english',\n",
       "    'dt__criterion': 'entropy'},\n",
       "   {'cvect__max_features': 100,\n",
       "    'cvect__stop_words': None,\n",
       "    'dt__criterion': 'gini'},\n",
       "   {'cvect__max_features': 100,\n",
       "    'cvect__stop_words': None,\n",
       "    'dt__criterion': 'entropy'},\n",
       "   {'cvect__max_features': 500,\n",
       "    'cvect__stop_words': 'english',\n",
       "    'dt__criterion': 'gini'},\n",
       "   {'cvect__max_features': 500,\n",
       "    'cvect__stop_words': 'english',\n",
       "    'dt__criterion': 'entropy'},\n",
       "   {'cvect__max_features': 500,\n",
       "    'cvect__stop_words': None,\n",
       "    'dt__criterion': 'gini'},\n",
       "   {'cvect__max_features': 500,\n",
       "    'cvect__stop_words': None,\n",
       "    'dt__criterion': 'entropy'},\n",
       "   {'cvect__max_features': 1000,\n",
       "    'cvect__stop_words': 'english',\n",
       "    'dt__criterion': 'gini'},\n",
       "   {'cvect__max_features': 1000,\n",
       "    'cvect__stop_words': 'english',\n",
       "    'dt__criterion': 'entropy'},\n",
       "   {'cvect__max_features': 1000,\n",
       "    'cvect__stop_words': None,\n",
       "    'dt__criterion': 'gini'},\n",
       "   {'cvect__max_features': 1000,\n",
       "    'cvect__stop_words': None,\n",
       "    'dt__criterion': 'entropy'},\n",
       "   {'cvect__max_features': 2000,\n",
       "    'cvect__stop_words': 'english',\n",
       "    'dt__criterion': 'gini'},\n",
       "   {'cvect__max_features': 2000,\n",
       "    'cvect__stop_words': 'english',\n",
       "    'dt__criterion': 'entropy'},\n",
       "   {'cvect__max_features': 2000,\n",
       "    'cvect__stop_words': None,\n",
       "    'dt__criterion': 'gini'},\n",
       "   {'cvect__max_features': 2000,\n",
       "    'cvect__stop_words': None,\n",
       "    'dt__criterion': 'entropy'}],\n",
       "  'split0_test_score': array([0.73926667, 0.7406    , 0.83123333, 0.83853333, 0.77176667,\n",
       "         0.77886667, 0.85166667, 0.8579    , 0.79023333, 0.798     ,\n",
       "         0.8599    , 0.86116667, 0.7988    , 0.8047    , 0.86493333,\n",
       "         0.86733333]),\n",
       "  'split1_test_score': array([0.73863333, 0.7395    , 0.8336    , 0.83796667, 0.7746    ,\n",
       "         0.78016667, 0.8508    , 0.85643333, 0.79303333, 0.79996667,\n",
       "         0.85836667, 0.86323333, 0.8019    , 0.809     , 0.86406667,\n",
       "         0.86816667]),\n",
       "  'split2_test_score': array([0.7396    , 0.7405    , 0.83346667, 0.83793333, 0.77573333,\n",
       "         0.78283333, 0.85236667, 0.85656667, 0.79393333, 0.8014    ,\n",
       "         0.85696667, 0.8604    , 0.8026    , 0.80906667, 0.86253333,\n",
       "         0.8645    ]),\n",
       "  'split3_test_score': array([0.7385    , 0.73963333, 0.83416667, 0.83503333, 0.77243333,\n",
       "         0.7791    , 0.849     , 0.85266667, 0.7921    , 0.79646667,\n",
       "         0.85563333, 0.8587    , 0.79986667, 0.80656667, 0.86076667,\n",
       "         0.86123333]),\n",
       "  'split4_test_score': array([0.74243333, 0.7435    , 0.83166667, 0.83386667, 0.7766    ,\n",
       "         0.78093333, 0.8535    , 0.86053333, 0.79316667, 0.79783333,\n",
       "         0.8568    , 0.86326667, 0.8087    , 0.81356667, 0.86253333,\n",
       "         0.86593333]),\n",
       "  'mean_test_score': array([0.73968667, 0.74074667, 0.83282667, 0.83666667, 0.77422667,\n",
       "         0.78038   , 0.85146667, 0.85682   , 0.79249333, 0.79873333,\n",
       "         0.85753333, 0.86135333, 0.80237333, 0.80858   , 0.86296667,\n",
       "         0.86543333]),\n",
       "  'std_test_score': array([0.00143149, 0.00144616, 0.00115651, 0.00185939, 0.00186063,\n",
       "         0.00143521, 0.00151745, 0.00254634, 0.00127129, 0.00173884,\n",
       "         0.00146727, 0.00174236, 0.00344541, 0.00297953, 0.00143481,\n",
       "         0.00244304]),\n",
       "  'rank_test_score': array([16, 15,  8,  7, 14, 13,  6,  5, 12, 11,  4,  3, 10,  9,  2,  1],\n",
       "        dtype=int32)},\n",
       " 1417)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = dt.now()\n",
    "\n",
    "grid_s_dt = GridSearchCV(pipe_stem_countvec_dt, param_grid=params_s_dt, cv=5)\n",
    "grid_s_dt.fit(X_train_s, y_train_s)\n",
    "acc_s = grid_s_dt.score(X_test_s, y_test_s)\n",
    "\n",
    "running_secs_dt = (dt.now() - start).seconds\n",
    "\n",
    "grid_s_dt.best_params_, grid_s_dt.best_score_,grid_s_dt.cv_results_, running_secs_dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_stem_tfidf_dt = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                ('dt', DecisionTreeClassifier())])\n",
    "pipe_stem_tfidf_dt.fit(X_train_lr, y_train_lr)\n",
    "test_acc_s_lr = pipe_stem_tfidf_dt.score(X_test_lr, y_test_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_s_lr = {'tfidf__max_features': [2000, 4000, 7000],\n",
    "         'tfidf__stop_words': ['english', None],\n",
    "              'dt__criterion': ['gini', 'entropy'],\n",
    "              'dt__max_depth': [2, 3, 5, 10, 20]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'dt__criterion': 'gini',\n",
       "  'dt__max_depth': 6,\n",
       "  'tfidf__max_features': 7000,\n",
       "  'tfidf__stop_words': None},\n",
       " 0.7850400000000001,\n",
       " {'mean_fit_time': array([1.40153594, 1.76983294, 1.41835399, 1.87697358, 1.62249241,\n",
       "         2.19045448, 1.67549906, 2.16936975, 1.74417038, 2.16049771,\n",
       "         1.73502588, 2.37050366]),\n",
       "  'std_fit_time': array([0.04386807, 0.01943262, 0.0130557 , 0.05669669, 0.11597427,\n",
       "         0.07174355, 0.07480921, 0.0902962 , 0.07710206, 0.06695675,\n",
       "         0.07785028, 0.08561773]),\n",
       "  'mean_score_time': array([0.2611074 , 0.27077603, 0.26168928, 0.28648162, 0.28124671,\n",
       "         0.32496138, 0.28095536, 0.30988789, 0.28768764, 0.29783177,\n",
       "         0.30477991, 0.32540088]),\n",
       "  'std_score_time': array([0.01183704, 0.00404138, 0.01185791, 0.0078617 , 0.01067146,\n",
       "         0.01657743, 0.01434522, 0.01629979, 0.0204475 , 0.02024357,\n",
       "         0.01733746, 0.00948767]),\n",
       "  'param_dt__criterion': masked_array(data=['gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                     'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                     'entropy'],\n",
       "               mask=[False, False, False, False, False, False, False, False,\n",
       "                     False, False, False, False],\n",
       "         fill_value='?',\n",
       "              dtype=object),\n",
       "  'param_dt__max_depth': masked_array(data=[6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6],\n",
       "               mask=[False, False, False, False, False, False, False, False,\n",
       "                     False, False, False, False],\n",
       "         fill_value='?',\n",
       "              dtype=object),\n",
       "  'param_tfidf__max_features': masked_array(data=[2000, 2000, 4000, 4000, 7000, 7000, 2000, 2000, 4000,\n",
       "                     4000, 7000, 7000],\n",
       "               mask=[False, False, False, False, False, False, False, False,\n",
       "                     False, False, False, False],\n",
       "         fill_value='?',\n",
       "              dtype=object),\n",
       "  'param_tfidf__stop_words': masked_array(data=['english', None, 'english', None, 'english', None,\n",
       "                     'english', None, 'english', None, 'english', None],\n",
       "               mask=[False, False, False, False, False, False, False, False,\n",
       "                     False, False, False, False],\n",
       "         fill_value='?',\n",
       "              dtype=object),\n",
       "  'params': [{'dt__criterion': 'gini',\n",
       "    'dt__max_depth': 6,\n",
       "    'tfidf__max_features': 2000,\n",
       "    'tfidf__stop_words': 'english'},\n",
       "   {'dt__criterion': 'gini',\n",
       "    'dt__max_depth': 6,\n",
       "    'tfidf__max_features': 2000,\n",
       "    'tfidf__stop_words': None},\n",
       "   {'dt__criterion': 'gini',\n",
       "    'dt__max_depth': 6,\n",
       "    'tfidf__max_features': 4000,\n",
       "    'tfidf__stop_words': 'english'},\n",
       "   {'dt__criterion': 'gini',\n",
       "    'dt__max_depth': 6,\n",
       "    'tfidf__max_features': 4000,\n",
       "    'tfidf__stop_words': None},\n",
       "   {'dt__criterion': 'gini',\n",
       "    'dt__max_depth': 6,\n",
       "    'tfidf__max_features': 7000,\n",
       "    'tfidf__stop_words': 'english'},\n",
       "   {'dt__criterion': 'gini',\n",
       "    'dt__max_depth': 6,\n",
       "    'tfidf__max_features': 7000,\n",
       "    'tfidf__stop_words': None},\n",
       "   {'dt__criterion': 'entropy',\n",
       "    'dt__max_depth': 6,\n",
       "    'tfidf__max_features': 2000,\n",
       "    'tfidf__stop_words': 'english'},\n",
       "   {'dt__criterion': 'entropy',\n",
       "    'dt__max_depth': 6,\n",
       "    'tfidf__max_features': 2000,\n",
       "    'tfidf__stop_words': None},\n",
       "   {'dt__criterion': 'entropy',\n",
       "    'dt__max_depth': 6,\n",
       "    'tfidf__max_features': 4000,\n",
       "    'tfidf__stop_words': 'english'},\n",
       "   {'dt__criterion': 'entropy',\n",
       "    'dt__max_depth': 6,\n",
       "    'tfidf__max_features': 4000,\n",
       "    'tfidf__stop_words': None},\n",
       "   {'dt__criterion': 'entropy',\n",
       "    'dt__max_depth': 6,\n",
       "    'tfidf__max_features': 7000,\n",
       "    'tfidf__stop_words': 'english'},\n",
       "   {'dt__criterion': 'entropy',\n",
       "    'dt__max_depth': 6,\n",
       "    'tfidf__max_features': 7000,\n",
       "    'tfidf__stop_words': None}],\n",
       "  'split0_test_score': array([0.6201    , 0.78436667, 0.62016667, 0.7843    , 0.61996667,\n",
       "         0.78516667, 0.61976667, 0.78096667, 0.61976667, 0.7811    ,\n",
       "         0.61976667, 0.78146667]),\n",
       "  'split1_test_score': array([0.62363333, 0.78396667, 0.62356667, 0.78373333, 0.62356667,\n",
       "         0.78346667, 0.62303333, 0.7818    , 0.623     , 0.78206667,\n",
       "         0.62296667, 0.78156667]),\n",
       "  'split2_test_score': array([0.62326667, 0.78696667, 0.62293333, 0.78813333, 0.62296667,\n",
       "         0.7891    , 0.623     , 0.7844    , 0.62303333, 0.78376667,\n",
       "         0.62293333, 0.7839    ]),\n",
       "  'split3_test_score': array([0.62473333, 0.78213333, 0.62456667, 0.78233333, 0.6241    ,\n",
       "         0.78173333, 0.62456667, 0.7782    , 0.6244    , 0.77803333,\n",
       "         0.6243    , 0.77773333]),\n",
       "  'split4_test_score': array([0.6254    , 0.7844    , 0.62533333, 0.78563333, 0.62496667,\n",
       "         0.78573333, 0.6252    , 0.7813    , 0.6251    , 0.7813    ,\n",
       "         0.62493333, 0.78076667]),\n",
       "  'mean_test_score': array([0.62342667, 0.78436667, 0.62331333, 0.78482667, 0.62311333,\n",
       "         0.78504   , 0.62311333, 0.78133333, 0.62306   , 0.78125333,\n",
       "         0.62298   , 0.78108667]),\n",
       "  'std_test_score': array([0.00182956, 0.0015433 , 0.00177546, 0.00196338, 0.0017051 ,\n",
       "         0.00246535, 0.00188073, 0.00197866, 0.00183344, 0.001864  ,\n",
       "         0.00178246, 0.00198176]),\n",
       "  'rank_test_score': array([ 7,  3,  8,  2,  9,  1, 10,  4, 11,  5, 12,  6], dtype=int32)},\n",
       " 131)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = dt.now()\n",
    "\n",
    "grid_tf_dt = GridSearchCV(pipe_stem_tfidf_dt, param_grid=params_s_lr)\n",
    "grid_tf_dt.fit(X_train_s, y_train_s)\n",
    "acc_s = grid_tf_dt.score(X_test_s, y_test_s)\n",
    "\n",
    "running_secs_dt = (dt.now() - start).seconds\n",
    "\n",
    "grid_tf_dt.best_params_, grid_tf_dt.best_score_,grid_tf_dt.cv_results_, running_secs_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate scores \n",
    "    # Training and test mean accuracy\n",
    "    #train_error = np.round(decision_tree.score(train_features, train_targets), 2)\n",
    "    #test_error = np.round(decision_tree.score(test_features, test_targets), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>best_params</th>\n",
       "      <th>best_score</th>\n",
       "      <th>test_time_sec</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic_stem</th>\n",
       "      <td>cvect__max_feature: 7000,cvect__stop_words:None</td>\n",
       "      <td>0.92305</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic_lemm</th>\n",
       "      <td>tfidf__max_features: 7000,tfidf__stop_words: None</td>\n",
       "      <td>0.91827</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecisionTree_stem</th>\n",
       "      <td>cvect__max_features: 2000,dt__criterion: entropy</td>\n",
       "      <td>0.86543</td>\n",
       "      <td>1471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecisionTree_lemm</th>\n",
       "      <td>criterion: gini,max_depth:6,max_features:7000</td>\n",
       "      <td>0.78504</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bayes_stem</th>\n",
       "      <td>cvect__max_features: 7000,cvect__stop_words: None</td>\n",
       "      <td>0.90633</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bayes_lemm</th>\n",
       "      <td>tfidf__max_features: 7000,tfidf__stop_words: None</td>\n",
       "      <td>0.90472</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         best_params  \\\n",
       "model                                                                  \n",
       "Logistic_stem        cvect__max_feature: 7000,cvect__stop_words:None   \n",
       "Logistic_lemm      tfidf__max_features: 7000,tfidf__stop_words: None   \n",
       "DecisionTree_stem   cvect__max_features: 2000,dt__criterion: entropy   \n",
       "DecisionTree_lemm      criterion: gini,max_depth:6,max_features:7000   \n",
       "Bayes_stem         cvect__max_features: 7000,cvect__stop_words: None   \n",
       "Bayes_lemm         tfidf__max_features: 7000,tfidf__stop_words: None   \n",
       "\n",
       "                  best_score test_time_sec  \n",
       "model                                       \n",
       "Logistic_stem        0.92305           153  \n",
       "Logistic_lemm        0.91827           132  \n",
       "DecisionTree_stem    0.86543          1471  \n",
       "DecisionTree_lemm    0.78504           131  \n",
       "Bayes_stem           0.90633            88  \n",
       "Bayes_lemm           0.90472            62  "
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'model': ['Logistic_stem', 'Logistic_lemm','DecisionTree_stem', 'DecisionTree_lemm',\n",
    "                        'Bayes_stem', 'Bayes_lemm'], \n",
    "             'best_params': ['cvect__max_feature: 7000,cvect__stop_words:None',\n",
    "                             'tfidf__max_features: 7000,tfidf__stop_words: None',\n",
    "                             'cvect__max_features: 2000,dt__criterion: entropy',\n",
    "                             'criterion: gini,max_depth:6,max_features:7000',\n",
    "                             'cvect__max_features: 7000,cvect__stop_words: None',\n",
    "                             'tfidf__max_features: 7000,tfidf__stop_words: None'],\n",
    "              \n",
    "             'best_score': ['0.92305', '0.91827', '0.86543', '0.78504', '0.90633', '0.90472'],\n",
    "             'test_time_sec': ['153','132','1471','131','88','62']}).set_index('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
